# üë• Analisador de Intera√ß√£o Social em Imagens

Este projeto utiliza um pipeline de Vis√£o Computacional para detectar pessoas em uma imagem e classificar seus pap√©is sociais prim√°rios como **"Falando"** ou **"Ouvindo"**. A an√°lise √© baseada em m√∫ltiplos fatores, incluindo pose corporal, express√µes faciais e dire√ß√£o do olhar.

A interface principal √© uma aplica√ß√£o web criada com Streamlit, que permite ao usu√°rio fazer o upload de uma imagem e visualizar o resultado da an√°lise de forma interativa.

## ‚ú® Funcionalidades

-   **Detec√ß√£o de M√∫ltiplas Pessoas:** Usa o modelo YOLOv8-Pose para localizar todas as pessoas na imagem.
-   **An√°lise de Pistas Visuais:** Extrai informa√ß√µes sobre gestos, abertura da boca e dire√ß√£o do olhar.
-   **Classifica√ß√£o de Papel Social:** Aplica uma l√≥gica heur√≠stica para determinar se uma pessoa est√° falando ou ouvindo.
-   **Interface Web Interativa:** Permite o upload f√°cil de imagens e a visualiza√ß√£o clara dos resultados.
-   **Execu√ß√£o Simplificada:** Um arquivo de lote (`run_app.bat`) permite iniciar a aplica√ß√£o com um duplo clique no Windows.

---

## üöÄ Como Executar o Projeto

### Pr√©-requisitos

-   Python 3.10+
-   Um ambiente virtual Python (o projeto foi configurado para usar um localizado em `../venv`).

### 1. Instala√ß√£o das Depend√™ncias

Todas as depend√™ncias necess√°rias est√£o listadas no arquivo `requirements.txt`. Para instal√°-las, use o `pip.exe` espec√≠fico do seu ambiente virtual. Estando na pasta ra√≠z do projeto (`Clas_Talk_Listsen`), execute o seguinte comando no terminal:


### 2. Iniciando a Aplica√ß√£o (M√©todo Recomendado)

Para facilitar o uso, foi criado um script de execu√ß√£o.

1.  Navegue at√© a pasta `social_vision_project`.
2.  D√™ um duplo clique no arquivo `run_app.bat`.

Isso iniciar√° o servidor Streamlit e abrir√° a aplica√ß√£o web no seu navegador padr√£o.

---

## ‚öôÔ∏è Como Funciona: O Pipeline de An√°lise

O resultado final √© gerado por um pipeline modular que executa 6 etapas em sequ√™ncia. Cada etapa √© implementada em seu pr√≥prio script Python.

1.  **`detector_pessoas_pose.py`**
    -   **O que faz:** Carrega a imagem e usa o modelo **YOLOv8-Pose** para detectar todas as pessoas presentes.
    -   **Sa√≠da:** Para cada pessoa, extrai a caixa delimitadora (bounding box) e os pontos-chave do esqueleto (keypoints).

2.  **`detector_faces.py`**
    -   **O que faz:** Para cada pessoa detectada, usa a biblioteca **`face_recognition`** (baseada no `dlib`) para localizar a regi√£o do rosto.
    -   **Sa√≠da:** Adiciona a caixa delimitadora do rosto aos dados da pessoa.

3.  **`expressao_boca_face_mesh.py`**
    -   **O que faz:** Utiliza o **MediaPipe Face Mesh** para mapear uma malha 3D detalhada sobre cada rosto. Analisa os pontos da boca para determinar se ela est√° aberta, um forte indicador de fala.
    -   **Sa√≠da:** Uma flag booleana `boca_aberta`.

4.  **`analise_pose_gestos.py`**
    -   **O que faz:** Analisa os keypoints do esqueleto (obtidos na etapa 1) para verificar se as m√£os da pessoa est√£o levantadas em uma posi√ß√£o de gesticula√ß√£o ativa.
    -   **Sa√≠da:** Uma flag booleana `gesticulando`.

5.  **`direcao_olhar.py`**
    -   **O que faz:** Estima a dire√ß√£o do olhar de cada pessoa com base na posi√ß√£o dos olhos e do nariz. Em seguida, calcula se esse "vetor de aten√ß√£o" est√° apontando para outra pessoa na cena.
    -   **Sa√≠da:** O ID da pessoa para quem o indiv√≠duo est√° olhando (`olhando_para_id`).

6.  **`classificador_social.py`**
    -   **O que faz:** Re√∫ne todas as pistas coletadas (`boca_aberta`, `gesticulando`, `olhando_para_id`) e aplica uma l√≥gica de decis√£o para classificar cada pessoa.
    -   **L√≥gica:** Uma pessoa com a boca aberta √© quase sempre "Falando". Uma pessoa que est√° olhando para quem fala √© classificada como "Ouvindo".
    -   **Sa√≠da:** O r√≥tulo final: "Falando" ou "Ouvindo".

---

## üß† Sobre os Modelos (Treinamento vs. Infer√™ncia)

√â fundamental entender que **este projeto n√£o realiza nenhum treinamento de modelos**. Ele exclusivamente utiliza modelos de aprendizado profundo que foram **pr√©-treinados** por outras equipes (Google, Ultralytics, etc.) em conjuntos de dados massivos. O processo que executamos √© chamado de **infer√™ncia**.

### Onde os modelos s√£o salvos?

As bibliotecas que usamos gerenciam o download e o armazenamento dos modelos de forma autom√°tica. Na primeira vez que o c√≥digo √© executado, os modelos s√£o baixados e salvos em um diret√≥rio de cache local no seu computador.

-   **YOLOv8 (`ultralytics`):** Salva os modelos (`.pt`) em `C:\Users\<Seu-Usuario>\AppData\Roaming\Ultralytics\models\`.
-   **MediaPipe (`mediapipe`):** Gerencia seus modelos (`.tflite`) internamente, geralmente dentro da pasta da biblioteca ou em um local de cache pr√≥prio.
-   **`face_recognition` (`dlib`):** Salva seus modelos (`.dat`) em um diret√≥rio de cache gerenciado pela biblioteca.

Esse mecanismo garante que os modelos sejam baixados apenas uma vez, tornando as execu√ß√µes futuras muito mais r√°pidas.

```